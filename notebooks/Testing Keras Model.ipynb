{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll be running tests on the model that I've designed, and improving logs and so forth for the final model.\n",
    "\n",
    "Below is the code from `model.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, re, random\n",
    "import sys, argparse, codecs\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    '''Parses all keyword arguments for model and returns them.\n",
    "\n",
    "       Returns:\n",
    "        - data_dir:   (str) The directory to the text file(s) for training.\n",
    "        - rnn_size:   (int) The number of cells in each hidden layer in \n",
    "                      the network.\n",
    "        - num_layers: (int) The number of hidden layers in the network.\n",
    "        - dropout:    (float) Dropout value (between 0, 1 exclusive).'''\n",
    "\n",
    "    # initialise parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments, set default values and expected types\n",
    "    parser.add_argument(\"-data_dir\",\n",
    "        help=\"The directory to the text file(s) for training.\")\n",
    "    parser.add_argument(\"-seq_length\", type=int, default=25,\n",
    "        help=\"The length of sequences to be used for training\")\n",
    "    parser.add_argument(\"-validation_split\", type=float, default=0.1,\n",
    "        help=\"The proportion of the training data to use for validation\")\n",
    "    parser.add_argument(\"-batch_size\", type=int, default=100,\n",
    "        help=\"The number of minibatches to be used for training\")\n",
    "    parser.add_argument(\"-rnn_size\", type=int, default=128,\n",
    "        help=\"The number of cells in each hidden layer in the network\")\n",
    "    parser.add_argument(\"-num_layers\", type=int, default=3,\n",
    "        help=\"The number of hidden layers in the network\")\n",
    "    parser.add_argument(\"-dropout\", type=float, default=0.1,\n",
    "        help=\"Dropout value (between 0, 1 exclusive)\")\n",
    "    parser.add_argument(\"-epochs\", type=int, default=20,\n",
    "        help=\"Number of epochs for training\")\n",
    "    parser.add_argument(\"-tensorboard\", type=int, default=1,\n",
    "        help=\"Save model statistics to tensorboard\")\n",
    "\n",
    "    # assert args.validation_split < 0.5\n",
    "    \n",
    "    # parse arguments and return their values\n",
    "    args = parser.parse_args()\n",
    "    return args.data_dir, args.seq_length, args.validation_split, \\\n",
    "           args.batch_size, args.rnn_size, args.num_layers, args.dropout, \\\n",
    "           args.epochs, args.tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_data(text):\n",
    "    '''Re-encodes text so that it can be printed to command line \n",
    "       without raising a UnicodeEncodeError, and then prints it.\n",
    "       Incompatible characters are simply dropped before printing.\n",
    "\n",
    "       Args:\n",
    "       - text: (str) The text to be printed'''\n",
    "\n",
    "    print(text.encode(sys.stdout.encoding, errors='replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_dir, encoding='utf-8'):\n",
    "    '''Appends all text files in data_dir into a single string and returns it.\n",
    "       All files are assumed to be utf-8 encoded, and of type '.txt'.\n",
    "\n",
    "       Args:\n",
    "       - data_dir: (str) The directory to text files for training.\n",
    "       - encoding: (str) The type of encoding to use when decoding each file.\n",
    "\n",
    "       Returns:\n",
    "       - text_data: (str) Appended files as a single string.'''\n",
    "\n",
    "    print(\"Loading data from %s\" % os.path.abspath(data_dir))\n",
    "    # Initialise text string\n",
    "    text_data = ''\n",
    "    # select .txt files from data_dir\n",
    "    for filename in filter(lambda s: s.endswith(\".txt\"), os.listdir(data_dir)):\n",
    "        # open file with default encoding\n",
    "        print(\"loading file: %s\" % filename)\n",
    "        filepath = os.path.abspath(os.path.join(data_dir, filename))\n",
    "        with open(filepath,'r', encoding = encoding) as f:\n",
    "            text_data += f.read() + \"\\n\"\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text_data, seq_length):\n",
    "    '''Preprocesses text_data for RNN model.\n",
    "\n",
    "       Args:\n",
    "       - text: (str) text file to be processed.\n",
    "       - seq_length: (int) length of character sequences to be considered \n",
    "                     in the training set.\n",
    "\n",
    "       Returns:\n",
    "       - char_to_int: (dict) Maps characters in the character set to ints.\n",
    "       - int_to_char: (dict) Maps ints to characters in the character set.\n",
    "       - n_chars: (int) The number of characters in the text.\n",
    "       - n_vocab: (int) The number of unique characters in the text.'''\n",
    "\n",
    "    # create mapping of unique chars to integers, and a reverse mapping\n",
    "    chars = sorted(set(text_data))\n",
    "    char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "    int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(text_data)\n",
    "    n_vocab = len(chars)\n",
    "    \n",
    "    return char_to_int, int_to_char, n_chars, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(batch, starts, text_data, seq_length, batch_size, \n",
    "              char_to_int, n_vocab):\n",
    "    '''A generator that returns sequences of length seq_length, in\n",
    "       batches of size batch_size.\n",
    "       \n",
    "       Args:\n",
    "       - batch: (int) The index of the batch to be returned\n",
    "       - text_data: (str) The text to feed the model\n",
    "       - seq_length: (int) The length of each training sequence\n",
    "       - batch_size: (int) The size of minibatches for training'''\n",
    "    \n",
    "    # prepare the dataset of input to output pairs encoded as integers\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for start in range(batch_size * batch, batch_size * (batch + 1)): \n",
    "        seq_in  = text_data[starts[start]:starts[start] + seq_length]\n",
    "        seq_out = text_data[starts[start] + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "        \n",
    "    X = np_utils.to_categorical(dataX, num_classes=n_vocab)\n",
    "    X = X.reshape(batch_size, seq_length, n_vocab)\n",
    "\n",
    "    # one hot encode the output variable\n",
    "    y = np_utils.to_categorical(dataY, num_classes=n_vocab)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_batches(mode, text_data, seq_length, validation_split,\n",
    "                     batch_size, char_to_int, n_chars, n_vocab,\n",
    "                     random_seed=1234, shuffle=True):\n",
    "    '''A generator that returns training sequences of length seq_length, in\n",
    "       batches of size batch_size.\n",
    "\n",
    "       Args:\n",
    "       - mode: (str) Whether the batch is for training or validation. \n",
    "               'validation' or 'train' only\n",
    "       - text_data: (str) The text for training\n",
    "       - seq_length: (int) The length of each training sequence\n",
    "       - batch_size: (int) The size of minibatches for training\n",
    "       - validation_split: (float) The proportion of batches to use as \n",
    "                           validation data\n",
    "       - random_seed: A random seed'''\n",
    "\n",
    "    # set random seed\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # index the text_data\n",
    "    starts = list(range(n_chars - n_chars % seq_length - seq_length))\n",
    "    \n",
    "    if shuffle:\n",
    "        # shuffle the indices\n",
    "        random.shuffle(starts)\n",
    "    \n",
    "#     while True:\n",
    "    \n",
    "    n_batches = n_chars // batch_size\n",
    "    validation_size = round(n_batches * validation_split)\n",
    "    if mode == 'validation':\n",
    "        for batch in range(validation_size):\n",
    "            X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                             batch_size, char_to_int, n_vocab)\n",
    "            yield X, y\n",
    "            \n",
    "    elif mode == 'train':\n",
    "        for batch in range(validation_size, n_batches):\n",
    "            X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                             batch_size, char_to_int, n_vocab)\n",
    "            yield X, y\n",
    "    else:\n",
    "        raise ValueError(\"only 'validation' and 'train' modes accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(batch_size, seq_length, n_vocab, \n",
    "                rnn_size, num_layers, drop_prob):\n",
    "    '''Defines the RNN LSTM model.\n",
    "\n",
    "       Args:\n",
    "        - batch_size: (int) The size of each minibatches.\n",
    "        - seq_length: (int) The length of each sequence for the model.\n",
    "        - rnn_size: (int) The number of cells in each hidden layer.\n",
    "        - num_layers: (int) The number of hidden layers in the network.\n",
    "        - drop_prob: (float) The proportion of cells to drop in each dropout \n",
    "                             layer.\n",
    "       Returns:\n",
    "        - model: (keras.models.Sequential) The constructed Keras model.'''\n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            # add last hidden layer\n",
    "            model.add(LSTM(rnn_size, return_sequences=False))\n",
    "        elif i == 0:\n",
    "            # add first hidden layer\n",
    "            model.add(LSTM(rnn_size, \n",
    "                           batch_input_shape=(None, seq_length, n_vocab),\n",
    "                           return_sequences=True))\n",
    "        else:\n",
    "            # add middle hidden layer\n",
    "            model.add(LSTM(rnn_size, return_sequences=True))\n",
    "        \n",
    "        model.add(Dropout(drop_prob))\n",
    "    # add output layer\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metric=['accuracy'])  \n",
    "\n",
    "    return model\n",
    "\n",
    "    # build and compile Keras model\n",
    "    model = build_model(batch_size, seq_length, n_vocab,\n",
    "                        rnn_size, num_layers, drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_callbacks(verbose, use_tensorboard):\n",
    "    '''Set callbacks for Keras model.\n",
    "\n",
    "    Args:\n",
    "     - use_tensorboard: (int) Add TensorBoard callback if use_tensorboard == 1\n",
    "\n",
    "    Returns:\n",
    "     - callbacks: (list) list of callbacks for model'''\n",
    "\n",
    "    callbacks = [ModelCheckpoint(\n",
    "                    r'..\\checkpoints\\weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                    verbose=verbose)]\n",
    "    if use_tensorboard:\n",
    "        tb_callback = TensorBoard(log_dir=r'..\\logs', histogram_freq=0.01,\n",
    "                              write_images=True)\n",
    "        callbacks.append(tb_callback)  \n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, text_data, seq_length, validation_split, epochs, \n",
    "              batch_size, char_to_int, n_chars, n_vocab, verbose, use_tensorboard):\n",
    "    '''Trains the model on the training data.\n",
    "\n",
    "       Args:\n",
    "       - model:\n",
    "       - text_data:\n",
    "       - seq_length:\n",
    "       - batch_size:\n",
    "       - char_to_int:'''\n",
    "    n_batches = len(text_data) // batch_size\n",
    "    batch_params = (text_data, seq_length, validation_split,\n",
    "                     batch_size, char_to_int, n_chars, n_vocab)\n",
    "    hist = model.fit_generator(\n",
    "               generator = generate_batches('train', *batch_params),\n",
    "               validation_data = generate_batches('validation', *batch_params),\n",
    "               validation_steps = int(n_batches * validation_split),\n",
    "               epochs = epochs,\n",
    "               steps_per_epoch = n_batches,\n",
    "               verbose = verbose,\n",
    "               callbacks = set_callbacks(verbose, use_tensorboard))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Main():\n",
    "    '''Executes the model'''\n",
    "\n",
    "    # load text data to memory\n",
    "    text_data = load_data(data_dir)\n",
    "\n",
    "    # preprocess the text - construct character dictionaries etc\n",
    "    char_to_int, int_to_char, n_chars, n_vocab = \\\n",
    "                                process_text(text_data, seq_length)\n",
    "\n",
    "    # build and compile Keras model\n",
    "    model = build_model(batch_size, seq_length, n_vocab,\n",
    "                        rnn_size, num_layers, drop_prob)\n",
    "\n",
    "    # fit model using generator\n",
    "    hist = fit_model(model, text_data, seq_length, validation_split, epochs,\n",
    "                   batch_size, char_to_int, n_chars, n_vocab, use_tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir, seq_length, validation_split, batch_size, rnn_size, \\\n",
    "    num_layers, drop_prob, epochs, verbose, use_tensorboard = \\\n",
    "        (r'..\\data', 25, 0.1, 100, 128, 3, 0.1, 10, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\caleb\\Documents\\Data Science\\welcome-to-night-vale\\data\n",
      "loading file: Welcome To Night Vale.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2250: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name lstm_1/kernel:0 is illegal; using lstm_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1/kernel:0 is illegal; using lstm_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1/recurrent_kernel:0 is illegal; using lstm_1/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1/recurrent_kernel:0 is illegal; using lstm_1/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1/bias:0 is illegal; using lstm_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_1/bias:0 is illegal; using lstm_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/kernel:0 is illegal; using lstm_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/kernel:0 is illegal; using lstm_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/recurrent_kernel:0 is illegal; using lstm_2/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/recurrent_kernel:0 is illegal; using lstm_2/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/bias:0 is illegal; using lstm_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_2/bias:0 is illegal; using lstm_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/kernel:0 is illegal; using lstm_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/kernel:0 is illegal; using lstm_3/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/recurrent_kernel:0 is illegal; using lstm_3/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/recurrent_kernel:0 is illegal; using lstm_3/recurrent_kernel_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/bias:0 is illegal; using lstm_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name lstm_3/bias:0 is illegal; using lstm_3/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "Epoch 1/10\n",
      "    9/16284 [..............................] - ETA: 17029s - loss: 5.2155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-15e8ca6c22c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m hist = fit_model(model, text_data, seq_length, validation_split, epochs,\n\u001b[1;32m     12\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_to_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                      verbose, use_tensorboard)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-780f17a22e57>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, text_data, seq_length, validation_split, epochs, batch_size, char_to_int, n_chars, n_vocab, verbose, use_tensorboard)\u001b[0m\n\u001b[1;32m     19\u001b[0m                \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                callbacks = set_callbacks(verbose, use_tensorboard))\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\caleb\\AppData\\Local\\conda\\conda\\envs\\tensorflow-env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load text data to memory\n",
    "text_data = load_data(data_dir)\n",
    "\n",
    "# preprocess the text - construct character dictionaries etc\n",
    "char_to_int, int_to_char, n_chars, n_vocab = process_text(text_data, seq_length)\n",
    "\n",
    "# build and compile Keras model\n",
    "model = build_model(batch_size, seq_length, n_vocab,\n",
    "                    rnn_size, num_layers, drop_prob)\n",
    "\n",
    "hist = fit_model(model, text_data, seq_length, validation_split, epochs,\n",
    "                     batch_size, char_to_int, n_chars, n_vocab,  \n",
    "                     verbose, use_tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = list(generate_batches('train', text_data, seq_length, \n",
    "                   validation_split, batch_size, char_to_int, n_chars, \n",
    "                   n_vocab, random_seed=1234))\n",
    "\n",
    "valid_batches = list(generate_batches('validation', text_data, seq_length, \n",
    "                   validation_split, batch_size, char_to_int, n_chars, \n",
    "                   n_vocab, random_seed=1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_results = []\n",
    "for inputs, target in train_batches:\n",
    "    inputs = (inputs + 1) * n_vocab / 2\n",
    "    inputs = inputs.astype(np.int32)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        seq = inputs[batch, :, :].reshape(seq_length)\n",
    "        seq_text = ''.join([int_to_char[x] for x in seq])\\\n",
    "                     .replace(\"\\n\", \" \")\n",
    "        targ = int_to_char[np.argmax(target, axis=1)[batch]]\n",
    "        targ = targ.replace(\"\\n\", \" \")\n",
    "        train_results.append(seq_text + targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_results = []\n",
    "for inputs, target in valid_batches:\n",
    "    inputs = (inputs + 1) * n_vocab / 2\n",
    "    inputs = inputs.astype(np.int32)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        seq = inputs[batch, :, :].reshape(seq_length)\n",
    "        seq_text = ''.join([int_to_char[x] for x in seq])\\\n",
    "                     .replace(\"\\n\", \" \")\n",
    "        targ = int_to_char[np.argmax(target, axis=1)[batch]]\n",
    "        targ = targ.replace(\"\\n\", \" \")\n",
    "        valid_results.append(seq_text + targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to implement the shuffling for the training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' them at gunpoint – that t',\n",
       " 'u-  But there    a man-  T',\n",
       " 'g wares+ the representativ',\n",
       " ' dives and loops around+ u',\n",
       " ' The only hints can be fou',\n",
       " 'ther army was marching    ',\n",
       " 'And then+ there was   + as',\n",
       " 'l expression for kids- But',\n",
       " 'l perpetrators saying thin',\n",
       " 'malevolent spirits+ or tan',\n",
       " 'ntact lenses you put in th',\n",
       " 't impossible to look bad i',\n",
       " 'g a religious holiday<”  T',\n",
       " \"'like in the chanted blood\",\n",
       " 'filled hugs- Ve had unackn',\n",
       " 'tly involved in the recove',\n",
       " 'he said this wasn’t always',\n",
       " 'on! Play Ball is only a fr',\n",
       " ' is over+ dear listeners- ',\n",
       " 'move along like nothing ha']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t apart+ toes together- Ri',\n",
       " 'y…what<…within the cave- A',\n",
       " 'w what art really is- You ',\n",
       " 'r word for the discreet bo',\n",
       " 'ud with a ladder going up ',\n",
       " 'es- Finer words were never',\n",
       " 'color of the universe+ and',\n",
       " 'anding in the center of th',\n",
       " 's for a lot- That counts f',\n",
       " 's I do not own yellow galo',\n",
       " 'ng out in the desert-  “Vh',\n",
       " 'rtwined in defiance of our',\n",
       " 'y- Representatives for the',\n",
       " ' Rtreet- “You know+ the ki',\n",
       " 't gave us more time to our',\n",
       " 'nza+ panache+ elephantitis',\n",
       " 'OK+ I won’t!   Con’t tell!',\n",
       " 'o you- Ge is innocent+ and',\n",
       " 'that history is a myth+ an',\n",
       " 'sion+ eyes- Ve’ve also rec']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialisation: bbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "['&', 'Т', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', '͊', ' ', ' ', 'з', 'з', 'з', 'з', 'з', 'з']\n",
      "&Т͊͊͊͊͊͊͊͊͊͊͊͊͊͊͊  зззззз\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "def sample_model(model, sample_length, n_vocab):\n",
    "    '''Prints out a sequence of text generated from the text data'''\n",
    "    \n",
    "    feed_seq = random.sample(range(n_vocab), seq_length)\n",
    "    print('initialisation:', ''.join([int_to_char[x] for x in feed_seq]))\n",
    "    feed_seq = [normalize_ints(i, n_vocab) for i in feed_seq]\n",
    "    for _ in range(sample_length):\n",
    "        feed_seq = np.reshape(feed_seq, (1, seq_length, 1))\n",
    "        next_char = model.predict(feed_seq, batch_size=1)\n",
    "        feed_seq = np.reshape(feed_seq, seq_length)\n",
    "        feed_seq = list(feed_seq)[1:] + [np.argmax(next_char)]\n",
    "    \n",
    "    out_seq = [int_to_char[round(x)] for x in feed_seq]\n",
    "    print(out_seq)\n",
    "    print(\"\".join(out_seq).replace(\"\\n\", \"\\\\n\"))\n",
    "    print(len(out_seq))\n",
    "\n",
    "sample_model(model, 25, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_in = [normalize_ints(i, n_vocab) for i in random.sample(range(n_vocab), 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Т'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_char[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_in = np.reshape(seq_in, (1, seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = model.predict(np.reshape(seq_in, (1, seq_length, 1)), batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1054, 4, 1, 29, 3, 1, 3296, 1970, 1, 21, 109498, 1, 1391, 5, 1, 1, 64, 19578, 1457, 1933, 79862, 5, 10, 1, 196, 1, 6, 4, 30354, 6, 4, 5, 2, 2, 3, 3, 3, 1, 1, 270480, 28604, 1, 37521, 1642, 3, 5, 5, 834, 106, 4, 160, 16, 1, 2, 119, 2316, 4, 41, 2, 28002, 4, 23, 3702, 4, 6, 10, 1208, 3, 802, 6, 1, 2, 415, 152, 1, 130, 27, 1, 1016, 4, 3, 1, 832, 76484, 19392, 1933, 7, 6, 2867, 788, 97433, 5, 1, 38093, 6804, 4, 1, 1, 11, 4, 6, 1, 2216, 2, 2116, 1, 3, 1378, 298, 49, 2, 1, 5, 4, 97646, 2, 15, 28235, 1, 1, 444, 553, 3, 2, 25274, 4, 832, 6, 2249, 54656, 4, 1, 1, 4, 1539, 1, 622, 3, 2, 3, 6, 1, 6, 106, 76, 3, 1, 5, 560, 79, 4, 1146, 1, 2, 196, 2, 2, 445, 23075, 71653, 3, 104, 4, 1, 4692, 1, 3, 9, 13, 2, 1813, 66583, 1, 3, 2, 2569, 3, 1806, 3105, 80, 2, 4, 1, 1, 17479, 2, 1, 3, 4, 3, 1418, 2, 1, 1840, 7591, 5, 11956, 1, 4, 5, 2, 2, 1, 2, 1417, 2, 4, 11900, 21831, 2, 2, 1, 3, 4, 4, 150348, 86679, 46722, 1, 3, 7, 18, 2, 4, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = sorted(set(text_data))\n",
    "Counter(text_data).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
